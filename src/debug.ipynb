{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'title': '2021年夏季无袖连衣裙酒红色高领', 'key_attr': {'袖长': '无袖', '领型': '高领'}, 'match': {'图文': 1, '袖长': 1, '领型': 1}}\n",
      "{'领型': [['高领', '半高领', '立领'], ['连帽', '可脱卸帽'], ['翻领', '衬衫领', 'POLO领', '方领', '娃娃领', '荷叶领'], ['双层领'], ['西装领'], ['U型领'], ['一字领'], ['围巾领'], ['堆堆领'], ['V领'], ['棒球领'], ['圆领'], ['斜领'], ['亨利领']], '袖长': [['短袖', '五分袖'], ['九分袖', '长袖'], ['七分袖'], ['无袖']], '衣长': [['超短款', '短款', '常规款'], ['长款', '超长款'], ['中长款']], '版型': [['修身型', '标准型'], ['宽松型']], '裙长': [['短裙', '超短裙'], ['中裙', '中长裙'], ['长裙']], '穿着方式': [['套头'], ['开衫']], '类别': [['手提包'], ['单肩包'], ['斜挎包'], ['双肩包']], '裤型': [['O型裤', '锥形裤', '哈伦裤', '灯笼裤'], ['铅笔裤', '直筒裤', '小脚裤'], ['工装裤'], ['紧身裤'], ['背带裤'], ['喇叭裤', '微喇裤'], ['阔腿裤']], '裤长': [['短裤'], ['五分裤'], ['七分裤'], ['九分裤', '长裤']], '裤门襟': [['松紧'], ['拉链'], ['系带']], '闭合方式': [['松紧带'], ['拉链'], ['套筒', '套脚', '一脚蹬'], ['系带'], ['魔术贴'], ['搭扣']], '鞋帮高度': [['高帮', '中帮'], ['低帮']]}\n"
     ]
    }
   ],
   "source": [
    "# 测试文本替换\n",
    "from replace import replace_entry,attr_dict\n",
    "data_entry = {\"title\": \"2021年夏季无袖连衣裙酒红色立领\", \"key_attr\": {\"袖长\": \"无袖\", \"领型\": \"立领\"}, \"match\": {\"图文\": 1, \"袖长\": 1, \"领型\": 1}}\n",
    "tmp = replace_entry(data_entry)\n",
    "print(tmp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'领型': ['高领', '连帽', '翻领', '双层领', '西装领', 'U型领', '一字领', '围巾领', '堆堆领', 'V领', '棒球领', '圆领', '斜领', '亨利领'], '袖长': ['短袖', '九分袖', '七分袖', '无袖'], '衣长': ['超短款', '长款', '中长款'], '版型': ['修身型', '宽松型'], '裙长': ['短裙', '中裙', '长裙'], '穿着方式': ['套头', '开衫'], '类别': ['手提包', '单肩包', '斜挎包', '双肩包'], '裤型': ['O型裤', '铅笔裤', '工装裤', '紧身裤', '背带裤', '喇叭裤', '阔腿裤'], '裤长': ['短裤', '五分裤', '七分裤', '九分裤'], '裤门襟': ['松紧', '拉链', '系带'], '闭合方式': ['松紧带', '拉链', '套筒', '系带', '魔术贴', '搭扣'], '鞋帮高度': ['高帮', '低帮']}\n",
      "dict_keys(['领型', '袖长', '衣长', '版型', '裙长', '穿着方式', '类别', '裤型', '裤长', '裤门襟', '闭合方式', '鞋帮高度'])\n"
     ]
    }
   ],
   "source": [
    "tmp_dict ={}\n",
    "for attr_name,attr_vals in attr_dict.items():\n",
    "    tmp_list = []\n",
    "    for attr_val in attr_vals:\n",
    "        tmp_list.append(attr_val[0])\n",
    "    tmp_dict[attr_name] = tmp_list\n",
    "print(tmp_dict)\n",
    "print(tmp_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50000it [00:58, 858.63it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "negative sample size:47008,positive_sample size:50000,ratio:0.000\n"
     ]
    }
   ],
   "source": [
    "from gen_neg_sample import generate_neg_samples\n",
    "\n",
    "generate_neg_samples(data_path =\"data/train_fine.txt\",res_data_dir=\"data/neg_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"data/train_coarse.txt\",\"r\",encoding=\"utf-8\") as f:\n",
    "    cnt = 0\n",
    "    for line in f:\n",
    "        t = json.dumps(line)\n",
    "        print(t)\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.94016\n"
     ]
    }
   ],
   "source": [
    "print(47008/50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "97008it [00:02, 37344.79it/s]\n",
      "  0%|          | 1/97008 [00:00<26:07, 61.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021年春季喇叭裤牛仔裤蓝色常规厚度九分裤女装\n",
      "24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 测试dataset\n",
    "# from model import Model\n",
    "from dataset import TrainDataSet\n",
    "from tqdm import  tqdm\n",
    "# model = Model()\n",
    "# tokenize = lambda x:model.tokenizer(x)\n",
    "input_filename =\"data/neg_data/text_data.txt\"\n",
    "img_dict_path = \"data/neg_data/img_dict.txt\"\n",
    "tokenize = lambda x:x\n",
    "ds = TrainDataSet(input_filename,img_dict_path,tokenize,is_train =True)\n",
    "max_text_len = 0\n",
    "cnt = 0\n",
    "for d in tqdm(ds):\n",
    "    if cnt>=1:break\n",
    "    img_feature,text,label = d\n",
    "    max_text_len = max(max_text_len,len(text))\n",
    "    print(text)\n",
    "    cnt+=1\n",
    "print(max_text_len) # 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 97008/97008 [00:07<00:00, 12370.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "max_text_len = 0\n",
    "for d in tqdm(ds):\n",
    "    img_feature,text,label = d\n",
    "    max_text_len = max(max_text_len,len(text))\n",
    "print(max_text_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "97008it [00:01, 91778.76it/s] \n",
      "  0%|          | 1/97008 [00:00<1:06:12, 24.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "img_feature torch.Size([2048])\n",
      "text {'input_ids': tensor([[ 101, 9960, 2399, 3217, 2108, 1589, 1375, 6175, 4281,  798, 6175, 5905,\n",
      "         5682, 2382, 6226, 1331, 2428,  736, 1146, 6175, 1957, 6163,  102,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0]])}\n",
      "torch.Size([1, 32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 测试dataset\n",
    "from model import Model\n",
    "from dataset import TrainDataSet\n",
    "from tqdm import  tqdm\n",
    "model = Model()\n",
    "tokenize = lambda x:model.tokenize(x)\n",
    "input_filename =\"data/neg_data/text_data.txt\"\n",
    "img_dict_path = \"data/neg_data/img_dict.txt\"\n",
    "ds = TrainDataSet(input_filename,img_dict_path,tokenize,is_train =True)\n",
    "\n",
    "cnt = 0\n",
    "for d in tqdm(ds):\n",
    "    if cnt>=1:break\n",
    "    img_feature,text,label = d\n",
    "    print(\"img_feature\",img_feature.shape) # (bs,2048)\n",
    "    print(\"text\",text)\n",
    "    print(text[\"input_ids\"].shape) # (bs,1,32)\n",
    "    cnt+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1., -1.],\n",
      "        [ 2.,  2.],\n",
      "        [-1.,  3.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "t = [torch.Tensor([1,2,-1]),torch.Tensor([-1,2,3])]\n",
    "t = torch.stack(t,dim=1)\n",
    "print(t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.049684946647457e-05\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "e = 2000 - 800\n",
    "es = 1363 * 5 -800\n",
    "base_lr = 1e-4\n",
    "t = 0.5 * base_lr * (1 + np.cos(e/es * np.pi)) \n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ True, False, False, False, False],\n",
      "        [False,  True, False, False, False]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "a = torch.Tensor([[1,0,1,0,1],\n",
    "                [1,1,0,0,0]]) # (bs,class_num)\n",
    "# 1  0  1  0  1\n",
    "# 1  1  0  0  0\n",
    "\n",
    "# 1| 0  0  0  0\n",
    "# 0  1| 0  1  1\n",
    "b = torch.Tensor([[1,0,0,0,0],\n",
    "                [0,1,0,1,1]]) # (bs_class_num)\n",
    "c = (a==b).float()\n",
    "# print(torch.nonzero(a))\n",
    "# print(torch.nonzero(b))\n",
    "\n",
    "print(a+b==2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "\n",
      "  0%|          | 0/250 [00:00<?, ?it/s]\n",
      "  0%|          | 1/250 [00:00<03:01,  1.37it/s]\n",
      "  1%|          | 2/250 [00:01<02:52,  1.44it/s]\n",
      "  1%|          | 3/250 [00:02<02:46,  1.48it/s]\n",
      "  2%|▏         | 4/250 [00:02<02:46,  1.48it/s]\n",
      "  2%|▏         | 5/250 [00:03<03:07,  1.30it/s]\n",
      "  2%|▏         | 6/250 [00:04<03:30,  1.16it/s]\n",
      "  3%|▎         | 7/250 [00:05<03:44,  1.08it/s]\n",
      "  3%|▎         | 8/250 [00:06<03:47,  1.06it/s]\n",
      "  4%|▎         | 9/250 [00:07<03:51,  1.04it/s]\n",
      "  4%|▍         | 10/250 [00:08<03:53,  1.03it/s]\n",
      "  4%|▍         | 11/250 [00:09<03:53,  1.02it/s]\n",
      "  5%|▍         | 12/250 [00:10<03:54,  1.02it/s]\n",
      "  5%|▌         | 13/250 [00:11<03:53,  1.01it/s]\n",
      "  6%|▌         | 14/250 [00:12<03:53,  1.01it/s]\n",
      "  6%|▌         | 15/250 [00:13<03:53,  1.01it/s]\n",
      "  6%|▋         | 16/250 [00:14<03:53,  1.00it/s]\n",
      "  7%|▋         | 17/250 [00:16<04:15,  1.10s/it]\n",
      "  7%|▋         | 18/250 [00:17<04:45,  1.23s/it]\n",
      "  8%|▊         | 19/250 [00:19<05:01,  1.31s/it]\n",
      "  8%|▊         | 20/250 [00:20<05:02,  1.31s/it]\n",
      "  8%|▊         | 21/250 [00:21<04:34,  1.20s/it]\n",
      "  9%|▉         | 22/250 [00:22<04:12,  1.11s/it]\n",
      "  9%|▉         | 23/250 [00:23<04:00,  1.06s/it]\n",
      " 10%|▉         | 24/250 [00:24<03:48,  1.01s/it]\n",
      " 10%|█         | 25/250 [00:24<03:40,  1.02it/s]\n",
      " 10%|█         | 26/250 [00:25<03:35,  1.04it/s]\n",
      " 11%|█         | 27/250 [00:26<03:32,  1.05it/s]\n",
      " 11%|█         | 28/250 [00:27<03:30,  1.06it/s]\n",
      " 12%|█▏        | 29/250 [00:28<03:29,  1.06it/s]\n",
      " 12%|█▏        | 30/250 [00:29<03:27,  1.06it/s]\n",
      " 12%|█▏        | 31/250 [00:30<03:22,  1.08it/s]\n",
      " 13%|█▎        | 32/250 [00:31<03:20,  1.09it/s]\n",
      " 13%|█▎        | 33/250 [00:32<03:19,  1.09it/s]\n",
      " 14%|█▎        | 34/250 [00:33<03:18,  1.09it/s]\n",
      " 14%|█▍        | 35/250 [00:34<03:16,  1.09it/s]\n",
      " 14%|█▍        | 36/250 [00:35<03:14,  1.10it/s]\n",
      " 15%|█▍        | 37/250 [00:35<03:11,  1.11it/s]\n",
      " 15%|█▌        | 38/250 [00:36<03:12,  1.10it/s]\n",
      " 16%|█▌        | 39/250 [00:37<03:11,  1.10it/s]\n",
      " 16%|█▌        | 40/250 [00:38<03:10,  1.10it/s]\n",
      " 16%|█▋        | 41/250 [00:39<03:14,  1.07it/s]\n",
      " 17%|█▋        | 42/250 [00:40<03:15,  1.06it/s]\n",
      " 17%|█▋        | 43/250 [00:41<03:14,  1.07it/s]\n",
      " 18%|█▊        | 44/250 [00:42<03:11,  1.08it/s]\n",
      " 18%|█▊        | 45/250 [00:43<03:08,  1.09it/s]\n",
      " 18%|█▊        | 46/250 [00:44<03:06,  1.09it/s]\n",
      " 19%|█▉        | 47/250 [00:45<03:05,  1.10it/s]\n",
      " 19%|█▉        | 48/250 [00:46<03:03,  1.10it/s]\n",
      " 20%|█▉        | 49/250 [00:47<03:02,  1.10it/s]\n",
      " 20%|██        | 50/250 [00:47<03:01,  1.10it/s]\n",
      " 20%|██        | 51/250 [00:48<03:00,  1.10it/s]\n",
      " 21%|██        | 52/250 [00:49<02:58,  1.11it/s]\n",
      " 21%|██        | 53/250 [00:50<02:58,  1.10it/s]\n",
      " 22%|██▏       | 54/250 [00:51<03:01,  1.08it/s]\n",
      " 22%|██▏       | 55/250 [00:52<03:01,  1.07it/s]\n",
      " 22%|██▏       | 56/250 [00:53<02:59,  1.08it/s]\n",
      " 23%|██▎       | 57/250 [00:54<02:58,  1.08it/s]\n",
      " 23%|██▎       | 58/250 [00:55<02:56,  1.09it/s]\n",
      " 24%|██▎       | 59/250 [00:56<02:54,  1.09it/s]\n",
      " 24%|██▍       | 60/250 [00:57<02:53,  1.09it/s]\n",
      " 24%|██▍       | 61/250 [00:57<02:52,  1.10it/s]\n",
      " 25%|██▍       | 62/250 [00:58<02:52,  1.09it/s]\n",
      " 25%|██▌       | 63/250 [00:59<02:52,  1.09it/s]\n",
      " 26%|██▌       | 64/250 [01:00<02:50,  1.09it/s]\n",
      " 26%|██▌       | 65/250 [01:01<02:49,  1.09it/s]\n",
      " 26%|██▋       | 66/250 [01:02<02:48,  1.09it/s]\n",
      " 27%|██▋       | 67/250 [01:03<02:47,  1.09it/s]\n",
      " 27%|██▋       | 68/250 [01:04<02:47,  1.09it/s]\n",
      " 28%|██▊       | 69/250 [01:05<02:46,  1.09it/s]\n",
      " 28%|██▊       | 70/250 [01:06<02:45,  1.09it/s]\n",
      " 28%|██▊       | 71/250 [01:07<02:42,  1.10it/s]\n",
      " 29%|██▉       | 72/250 [01:08<02:42,  1.10it/s]\n",
      " 29%|██▉       | 73/250 [01:08<02:41,  1.10it/s]\n",
      " 30%|██▉       | 74/250 [01:09<02:38,  1.11it/s]\n",
      " 30%|███       | 75/250 [01:10<02:39,  1.10it/s]\n",
      " 30%|███       | 76/250 [01:11<02:40,  1.09it/s]\n",
      " 31%|███       | 77/250 [01:12<02:38,  1.09it/s]\n",
      " 31%|███       | 78/250 [01:13<02:37,  1.09it/s]\n",
      " 32%|███▏      | 79/250 [01:14<02:36,  1.09it/s]\n",
      " 32%|███▏      | 80/250 [01:15<02:36,  1.09it/s]\n",
      " 32%|███▏      | 81/250 [01:16<02:36,  1.08it/s]\n",
      " 33%|███▎      | 82/250 [01:17<02:36,  1.08it/s]\n",
      " 33%|███▎      | 83/250 [01:18<02:34,  1.08it/s]\n",
      " 34%|███▎      | 84/250 [01:19<02:32,  1.09it/s]\n",
      " 34%|███▍      | 85/250 [01:20<02:30,  1.10it/s]\n",
      " 34%|███▍      | 86/250 [01:20<02:28,  1.10it/s]\n",
      " 35%|███▍      | 87/250 [01:21<02:26,  1.11it/s]\n",
      " 35%|███▌      | 88/250 [01:22<02:25,  1.12it/s]\n",
      " 36%|███▌      | 89/250 [01:23<02:25,  1.10it/s]\n",
      " 36%|███▌      | 90/250 [01:24<02:23,  1.11it/s]\n",
      " 36%|███▋      | 91/250 [01:25<02:22,  1.12it/s]\n",
      " 37%|███▋      | 92/250 [01:26<02:22,  1.11it/s]\n",
      " 37%|███▋      | 93/250 [01:27<02:21,  1.11it/s]\n",
      " 38%|███▊      | 94/250 [01:28<02:22,  1.09it/s]\n",
      " 38%|███▊      | 95/250 [01:29<02:21,  1.10it/s]\n",
      " 38%|███▊      | 96/250 [01:29<02:21,  1.09it/s]\n",
      " 39%|███▉      | 97/250 [01:30<02:19,  1.10it/s]\n",
      " 39%|███▉      | 98/250 [01:31<02:19,  1.09it/s]\n",
      " 40%|███▉      | 99/250 [01:32<02:17,  1.09it/s]\n",
      " 40%|████      | 100/250 [01:33<02:18,  1.08it/s]\n",
      " 40%|████      | 101/250 [01:34<02:19,  1.07it/s]\n",
      " 41%|████      | 102/250 [01:35<02:18,  1.07it/s]\n",
      " 41%|████      | 103/250 [01:36<02:16,  1.08it/s]\n",
      " 42%|████▏     | 104/250 [01:37<02:15,  1.08it/s]\n",
      " 42%|████▏     | 105/250 [01:38<02:14,  1.08it/s]\n",
      " 42%|████▏     | 106/250 [01:39<02:14,  1.07it/s]\n",
      " 43%|████▎     | 107/250 [01:40<02:14,  1.07it/s]\n",
      " 43%|████▎     | 108/250 [01:41<02:13,  1.07it/s]\n",
      " 44%|████▎     | 109/250 [01:42<02:10,  1.08it/s]\n",
      " 44%|████▍     | 110/250 [01:42<02:08,  1.09it/s]\n",
      " 44%|████▍     | 111/250 [01:43<02:07,  1.09it/s]\n",
      " 45%|████▍     | 112/250 [01:44<02:05,  1.10it/s]\n",
      " 45%|████▌     | 113/250 [01:45<02:03,  1.11it/s]\n",
      " 46%|████▌     | 114/250 [01:46<02:03,  1.10it/s]\n",
      " 46%|████▌     | 115/250 [01:47<02:03,  1.09it/s]\n",
      " 46%|████▋     | 116/250 [01:48<02:02,  1.09it/s]\n",
      " 47%|████▋     | 117/250 [01:49<02:00,  1.10it/s]\n",
      " 47%|████▋     | 118/250 [01:50<02:00,  1.09it/s]\n",
      " 48%|████▊     | 119/250 [01:51<02:00,  1.09it/s]\n",
      " 48%|████▊     | 120/250 [01:52<01:59,  1.08it/s]\n",
      " 48%|████▊     | 121/250 [01:52<01:58,  1.09it/s]\n",
      " 49%|████▉     | 122/250 [01:53<01:57,  1.09it/s]\n",
      " 49%|████▉     | 123/250 [01:54<01:56,  1.09it/s]\n",
      " 50%|████▉     | 124/250 [01:55<01:54,  1.10it/s]\n",
      " 50%|█████     | 125/250 [01:56<01:54,  1.09it/s]\n",
      " 50%|█████     | 126/250 [01:57<01:53,  1.10it/s]\n",
      " 51%|█████     | 127/250 [01:58<01:51,  1.10it/s]\n",
      " 51%|█████     | 128/250 [01:59<01:50,  1.11it/s]\n",
      " 52%|█████▏    | 129/250 [02:00<01:49,  1.11it/s]\n",
      " 52%|█████▏    | 130/250 [02:01<01:48,  1.10it/s]\n",
      " 52%|█████▏    | 131/250 [02:02<01:49,  1.08it/s]\n",
      " 53%|█████▎    | 132/250 [02:03<01:48,  1.08it/s]\n",
      " 53%|█████▎    | 133/250 [02:03<01:48,  1.07it/s]\n",
      " 54%|█████▎    | 134/250 [02:04<01:48,  1.07it/s]\n",
      " 54%|█████▍    | 135/250 [02:05<01:47,  1.07it/s]\n",
      " 54%|█████▍    | 136/250 [02:06<01:44,  1.09it/s]\n",
      " 55%|█████▍    | 137/250 [02:07<01:43,  1.09it/s]\n",
      " 55%|█████▌    | 138/250 [02:08<01:42,  1.09it/s]\n",
      " 56%|█████▌    | 139/250 [02:09<01:41,  1.09it/s]\n",
      " 56%|█████▌    | 140/250 [02:10<01:40,  1.10it/s]\n",
      " 56%|█████▋    | 141/250 [02:11<01:40,  1.09it/s]\n",
      " 57%|█████▋    | 142/250 [02:12<01:38,  1.10it/s]\n",
      " 57%|█████▋    | 143/250 [02:13<01:37,  1.10it/s]\n",
      " 58%|█████▊    | 144/250 [02:14<01:36,  1.10it/s]\n",
      " 58%|█████▊    | 145/250 [02:14<01:35,  1.10it/s]\n",
      " 58%|█████▊    | 146/250 [02:15<01:35,  1.09it/s]\n",
      " 59%|█████▉    | 147/250 [02:16<01:35,  1.08it/s]\n",
      " 59%|█████▉    | 148/250 [02:17<01:34,  1.08it/s]\n",
      " 60%|█████▉    | 149/250 [02:18<01:32,  1.09it/s]\n",
      " 60%|██████    | 150/250 [02:19<01:32,  1.09it/s]\n",
      " 60%|██████    | 151/250 [02:20<01:31,  1.09it/s]\n",
      " 61%|██████    | 152/250 [02:21<01:29,  1.10it/s]\n",
      " 61%|██████    | 153/250 [02:22<01:28,  1.09it/s]\n",
      " 62%|██████▏   | 154/250 [02:23<01:27,  1.09it/s]\n",
      " 62%|██████▏   | 155/250 [02:24<01:27,  1.08it/s]\n",
      " 62%|██████▏   | 156/250 [02:25<01:26,  1.09it/s]\n",
      " 63%|██████▎   | 157/250 [02:26<01:25,  1.08it/s]\n",
      " 63%|██████▎   | 158/250 [02:26<01:25,  1.08it/s]\n",
      " 64%|██████▎   | 159/250 [02:27<01:24,  1.08it/s]\n",
      " 64%|██████▍   | 160/250 [02:28<01:23,  1.07it/s]\n",
      " 64%|██████▍   | 161/250 [02:29<01:23,  1.07it/s]\n",
      " 65%|██████▍   | 162/250 [02:30<01:27,  1.00it/s]\n",
      " 65%|██████▌   | 163/250 [02:32<01:42,  1.18s/it]\n",
      " 66%|██████▌   | 164/250 [02:34<01:51,  1.30s/it]\n",
      " 66%|██████▌   | 165/250 [02:35<01:56,  1.37s/it]\n",
      " 66%|██████▋   | 166/250 [02:37<01:57,  1.40s/it]\n",
      " 67%|██████▋   | 167/250 [02:38<01:59,  1.44s/it]\n",
      " 67%|██████▋   | 168/250 [02:40<02:00,  1.46s/it]\n",
      " 68%|██████▊   | 169/250 [02:41<01:59,  1.47s/it]\n",
      " 68%|██████▊   | 170/250 [02:43<01:57,  1.47s/it]\n",
      " 68%|██████▊   | 171/250 [02:44<01:55,  1.46s/it]\n",
      " 69%|██████▉   | 172/250 [02:45<01:52,  1.44s/it]\n",
      " 69%|██████▉   | 173/250 [02:46<01:40,  1.30s/it]\n",
      " 70%|██████▉   | 174/250 [02:47<01:30,  1.19s/it]\n",
      " 70%|███████   | 175/250 [02:48<01:23,  1.11s/it]\n",
      " 70%|███████   | 176/250 [02:49<01:17,  1.05s/it]\n",
      " 71%|███████   | 177/250 [02:50<01:13,  1.01s/it]\n",
      " 71%|███████   | 178/250 [02:51<01:11,  1.01it/s]\n",
      " 72%|███████▏  | 179/250 [02:52<01:09,  1.03it/s]\n",
      " 72%|███████▏  | 180/250 [02:53<01:07,  1.04it/s]\n",
      " 72%|███████▏  | 181/250 [02:54<01:05,  1.06it/s]\n",
      " 73%|███████▎  | 182/250 [02:55<01:03,  1.08it/s]\n",
      " 73%|███████▎  | 183/250 [02:56<01:01,  1.08it/s]\n",
      " 74%|███████▎  | 184/250 [02:57<01:01,  1.08it/s]\n",
      " 74%|███████▍  | 185/250 [02:57<00:59,  1.09it/s]\n",
      " 74%|███████▍  | 186/250 [02:58<00:57,  1.11it/s]\n",
      " 75%|███████▍  | 187/250 [02:59<00:57,  1.10it/s]\n",
      " 75%|███████▌  | 188/250 [03:00<00:56,  1.10it/s]\n",
      " 76%|███████▌  | 189/250 [03:01<00:55,  1.10it/s]\n",
      " 76%|███████▌  | 190/250 [03:02<00:54,  1.10it/s]\n",
      " 76%|███████▋  | 191/250 [03:03<00:53,  1.10it/s]\n",
      " 77%|███████▋  | 192/250 [03:04<00:53,  1.08it/s]\n",
      " 77%|███████▋  | 193/250 [03:05<00:52,  1.08it/s]\n",
      " 78%|███████▊  | 194/250 [03:06<00:51,  1.08it/s]\n",
      " 78%|███████▊  | 195/250 [03:07<00:50,  1.09it/s]\n",
      " 78%|███████▊  | 196/250 [03:07<00:49,  1.10it/s]\n",
      " 79%|███████▉  | 197/250 [03:08<00:48,  1.10it/s]\n",
      " 79%|███████▉  | 198/250 [03:09<00:47,  1.10it/s]\n",
      " 80%|███████▉  | 199/250 [03:10<00:46,  1.10it/s]\n",
      " 80%|████████  | 200/250 [03:11<00:45,  1.10it/s]\n",
      " 80%|████████  | 201/250 [03:12<00:44,  1.10it/s]\n",
      " 81%|████████  | 202/250 [03:13<00:43,  1.11it/s]\n",
      " 81%|████████  | 203/250 [03:14<00:42,  1.11it/s]\n",
      " 82%|████████▏ | 204/250 [03:15<00:41,  1.10it/s]\n",
      " 82%|████████▏ | 205/250 [03:16<00:41,  1.08it/s]\n",
      " 82%|████████▏ | 206/250 [03:17<00:40,  1.08it/s]\n",
      " 83%|████████▎ | 207/250 [03:18<00:39,  1.08it/s]\n",
      " 83%|████████▎ | 208/250 [03:18<00:38,  1.09it/s]\n",
      " 84%|████████▎ | 209/250 [03:19<00:37,  1.09it/s]\n",
      " 84%|████████▍ | 210/250 [03:20<00:36,  1.09it/s]\n",
      " 84%|████████▍ | 211/250 [03:21<00:35,  1.09it/s]\n",
      " 85%|████████▍ | 212/250 [03:22<00:34,  1.10it/s]\n",
      " 85%|████████▌ | 213/250 [03:23<00:33,  1.10it/s]\n",
      " 86%|████████▌ | 214/250 [03:24<00:32,  1.09it/s]\n",
      " 86%|████████▌ | 215/250 [03:25<00:31,  1.10it/s]\n",
      " 86%|████████▋ | 216/250 [03:26<00:30,  1.10it/s]\n",
      " 87%|████████▋ | 217/250 [03:27<00:30,  1.09it/s]\n",
      " 87%|████████▋ | 218/250 [03:28<00:29,  1.09it/s]\n",
      " 88%|████████▊ | 219/250 [03:29<00:28,  1.08it/s]\n",
      " 88%|████████▊ | 220/250 [03:29<00:27,  1.08it/s]\n",
      " 88%|████████▊ | 221/250 [03:30<00:26,  1.09it/s]\n",
      " 89%|████████▉ | 222/250 [03:31<00:25,  1.10it/s]\n",
      " 89%|████████▉ | 223/250 [03:32<00:24,  1.10it/s]\n",
      " 90%|████████▉ | 224/250 [03:33<00:23,  1.11it/s]\n",
      " 90%|█████████ | 225/250 [03:34<00:22,  1.10it/s]\n",
      " 90%|█████████ | 226/250 [03:35<00:21,  1.10it/s]\n",
      " 91%|█████████ | 227/250 [03:36<00:20,  1.10it/s]\n",
      " 91%|█████████ | 228/250 [03:37<00:19,  1.11it/s]\n",
      " 92%|█████████▏| 229/250 [03:38<00:18,  1.11it/s]\n",
      " 92%|█████████▏| 230/250 [03:38<00:18,  1.10it/s]\n",
      " 92%|█████████▏| 231/250 [03:39<00:17,  1.08it/s]\n",
      " 93%|█████████▎| 232/250 [03:40<00:16,  1.08it/s]\n",
      " 93%|█████████▎| 233/250 [03:41<00:15,  1.08it/s]\n",
      " 94%|█████████▎| 234/250 [03:42<00:14,  1.09it/s]\n",
      " 94%|█████████▍| 235/250 [03:43<00:13,  1.09it/s]\n",
      " 94%|█████████▍| 236/250 [03:44<00:12,  1.10it/s]\n",
      " 95%|█████████▍| 237/250 [03:45<00:11,  1.10it/s]\n",
      " 95%|█████████▌| 238/250 [03:46<00:10,  1.09it/s]\n",
      " 96%|█████████▌| 239/250 [03:47<00:10,  1.10it/s]\n",
      " 96%|█████████▌| 240/250 [03:48<00:09,  1.04it/s]\n",
      " 96%|█████████▋| 241/250 [03:49<00:08,  1.02it/s]\n",
      " 97%|█████████▋| 242/250 [03:50<00:08,  1.01s/it]\n",
      " 97%|█████████▋| 243/250 [03:51<00:07,  1.01s/it]\n",
      " 98%|█████████▊| 244/250 [03:52<00:05,  1.02it/s]\n",
      " 98%|█████████▊| 245/250 [03:53<00:04,  1.03it/s]\n",
      " 98%|█████████▊| 246/250 [03:54<00:03,  1.05it/s]\n",
      " 99%|█████████▉| 247/250 [03:55<00:02,  1.07it/s]\n",
      " 99%|█████████▉| 248/250 [03:56<00:01,  1.07it/s]\n",
      "100%|█████████▉| 249/250 [03:56<00:00,  1.07it/s]\n",
      "100%|██████████| 250/250 [03:57<00:00,  1.05it/s]\n",
      "100%|██████████| 250/250 [03:57<00:00,  1.05it/s]\n"
     ]
    }
   ],
   "source": [
    "!python main.py \\\n",
    "    --test-file=\"data/preliminary_testA.txt\" \\\n",
    "    --class-map=\"data/neg_data/label_map.txt\" \\\n",
    "    --batch-size=16 \\\n",
    "    --resume=\"../logs2022411/log/lr=0.0001_batch-size=64_date=2022-04-11-07-19-25/checkpoint/epoch_6.pt\" \\\n",
    "    --pred-res-path=\"../preds.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.0, 0.0, 1.0, 0.0], [1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0]]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "a = [torch.Tensor([1, 1, 1]), torch.Tensor([0, 0, 0]), torch.Tensor([1, 0, 0]), torch.Tensor([0, 0, 0]), ]\n",
    "a = torch.stack(a,dim=0) # (4*3)  (3*4)\n",
    "a = a.T\n",
    "a =a.tolist()\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'1': 1, '2': 2}]\n"
     ]
    }
   ],
   "source": [
    "tmp =[]\n",
    "a ={\"1\":1,\"2\":2}\n",
    "tmp.append(a)\n",
    "print(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8ee65edac6e753382670ef3d12ed394f5e8d8069bdfe339d3d2a95e0361efe5d"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('gaiic_track1')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
